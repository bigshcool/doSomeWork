# 线性回归梯度下降法

## 1. 基本概念

- **假设函数**

  假设模型(也成为假设函数，Hypothesis function)：就是根据特征变量（feature或者variable）拟合出目标变量（target variable）的公式或者函数。如下面的房价训练集表格，设置假设函数为h<sub>θ</sub>(x) = θ<sub>0</sub>+θ<sub>1</sub>x,其中θ<sub>0</sub>，θ<sub>1</sub>称为参数（parameter）。

  <center face="黑体" size = "10">训练集</center>

  | 面积（x） | 价格（y） |
  | --------- | --------- |
  | 2104      | 460       |
  | 1416      | 232       |
  | 1534      | 315       |
  | ...       | ...       |

  **根据中学知识我们知道θ<sub>0</sub>、θ<sub>1</sub>选择的 不同，将会影响整个图像趋势，那么面积对应的价格价格也会有所不同，为了预测的正确我要选择出合适的参数，让假设函数的预测值尽可能接近真实值，即形成一个良好的预测。**

- **代价函数**

  代价函数或者损失函数(Cost function或者Loss function)是用来评价假设模型（假设函数）的精确度。在已有训练集中，模型的拟合越好，代价函数就越小。在机器学习中，训练的目的就是要选择合适的参数（如上图中的θ<sub>0</sub>和θ<sub>1</sub>），让代价函数达到最小。换句话说，使用**x通过假设函数求出来的h<sub>θ</sub>(x)与真实y之间的误差要足够小。**

  例如，**在线性回顾中**，代价函数J(θ<sub>0</sub>、θ<sub>1</sub>)可理解为样本真实输出值和假设函数估算值之差平方和的平均值，采用方差进行定义。

  ![image](https://github.com/bigshcool/myPic/blob/main/cost_function.png)

- **梯度下降**

  梯度下降（Gradient descent）法是使代价函数达到最小的经典方法之一。将代价函数J(θ<sub>0</sub>、θ<sub>1</sub>)看成θ<sub>0</sub>,θ<sub>1</sub>组成的函数，函数可视化图像如下图所示。**学习的目标就是让找到合适的θ<sub>0</sub>,θ<sub>1</sub>,并且让代价函数的变得最小**

  ![image](https://github.com/bigshcool/myPic/blob/main/Gradient_descent.png)

  首先来看看梯度下降的一个直观的解释。比如我们在一座大山上的某处位置，由于我们不知道怎么下山，于是决定走一步算一步，也就是在每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山峰低处。

  从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。**当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解**，线性函数得到梯度下降法得到一定是全局最优解。

  批量梯度下降（batch gradient descent）算法的公式为:

  ![image](https://github.com/bigshcool/myPic/blob/main/batch_gradient_descent.png)

  其中α是学习率（**learning rate**），它决定了我们沿着能让**代价函数下降程度最大的方向向下迈出的步子有多大，如果α太大我们可能错过局部最优解甚至无法收敛，如果α太小，下降的速度就太低**，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。

  在梯度下降算法中，还有一个更微妙的问题，梯度下降中，我们需要更新θ<sub>0</sub>、θ<sub>1</sub>，而实现梯度下降法的精妙之处，在这个表达式中如果你需要更新灯饰，那么就需要同时更新θ<sub>0</sub>,θ<sub>1</sub>，具体更新公式如下。

  ![image-20220502222249730](https://github.com/bigshcool/myPic/blob/main/simultaneour_update.png)

  **具体算法需要通过临时变量来保证θ<sub>0</sub>、θ<sub>1</sub>同时更新，其实也非常好理解，假如您正在下山，我想判断是的当前位置从那个方向下降最快,如果你某一个参数发生了改变，势必会影响你在山上的位置，从而造成不合理的误差(当然不是说任何时候都需要保证同步更新)。这样看来同步更新其实使是一种更为自然的实现方式，当人们谈到梯度下降时，其实默认就是同步更新的模式**。

- 打